# Sparkify Data Warehouse in AWS Redshift

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to.

## AWS PREREQUSITES
1. An AWS Account with an IAM User with the following permissions
    - AdminstratorAccess
    - AmazonRedshiftFullAccess
    - AmazonS3FullAccess
2. Local AWS CLI (download here: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
    - Local .aws file will need to be configured (see: https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html#cli-configure-files-methods)

## Project Details
In this project, you'll apply what you've learned in [Udacity Data Engineering with AWS](https://www.udacity.com/enrollment/nd027) Nanodegree program section on Cloud Data Warehouses and ETL to build an ETL pipeline for a database hosted on Redshift. To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Datasets

**NOTE:** The udacity-dend S3 bucket is located in *us-west-2*

* s3://udacity-dend/song_data
* s3://udacity-dend/log_data

The first dataset (s3://udacity-dend/song_data) is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

> <p>song_data/A/B/C/TRABCEI128F424C983.json<br />
> song_data/A/A/B/TRAABJL12903CDCF1A.json</p>

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

> ``{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}``

The second dataset (s3://udacity-dend/log_data) consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are file paths to two files in this dataset.

> <p>log_data/2018/11/2018-11-12-events.json<br />
> log_data/2018/11/2018-11-13-events.json</p>

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![log file data](https://github.com/brettambrose/udacity-nanodegree-data-engineering-aws-project-redshift-data-warehouse/blob/master/images/log_data_example.jpg)

To properly read the log data, you'll need the following metadata file:

* s3://udacity-dend/log_json_path.json

The log_json_path.json file is used when loading JSON data into Redshift. It specifies the structure of the JSON data so that Redshift can properly parse and load it into the staging tables.

In the context of this project, you will need the log_json_path.json file to in the COPY command, which is responsible for loading the log data from S3 into staging tables in Redshift.  The log_json_path.json tells Redshift how to interprest the JSON data and extract the relevant fields. This is essential for further processing and transforming the data into the desired analytics tables.

Below is what data is in log_json_path.json

<pre>{"jsonpaths": [
        "$['artist']",
        "$['auth']",
        "$['firstName']",
        "$['gender']",
        "$['itemInSession']",
        "$['lastName']",
        "$['length']",
        "$['level']",
        "$['location']",
        "$['method']",
        "$['page']",
        "$['registration']",
        "$['sessionId']",
        "$['song']",
        "$['status']",
        "$['ts']",
        "$['userAgent']",
        "$['userId']"
    ]
}</pre>

## Star Schema Design
Using the song and event datasets, we will need to create a **star schema** optimized for queries on song play analysis.  This will include the following tables

**songplays** - The fact table that stores records in event data associated with song plays i.e. records with page "NextSong".
1. songplay_id - primary key, incrementally generated
2. start_time - sortkey, range filtering expected on this field
3. user_id
4. level
5. song_id
6. artist_id - distkey, commonly joined field for expected use case
7. session_id
8. location
9. user_agent

**users** - Dimension table for users in the app
1. user_id - primary key
2. first_name
3. last_name
4. gender
5. level - sortkey, filtering expected on this field

**songs** - Dimension table for songs in the music database
1. song_id - primary key
2. title
3. artist_id - distkey, commonly joined field for expected use case
4. year
5. duration

**artists** - Dimension table for artists in the music database
1. artist_id - primary key & distkey, commonly joined field for expected use case
2. name
3. location
4. latitude
5. longitude

**time** - Dimension table timestamps of records in songplays, with enriched datetime selections
1. start_time
2. hour
3. day
4. week
5. month
6. year
7. weekday - sortkey, filtering expected on this field

### Common Use Cases
We can anticipate some common queries business analysts might construct for reporting, which will help determine optimal table design (sortkeys and distkeys).  Some queries may queries may include the following:

**Getting a list of artists sorted in descending order by most played songs**
```
SELECT
 a.name AS artist_name,
 COUNT(sp.songplay_id) AS songplay_count
FROM songplays sp
JOIN artists a
 ON (sp.artist_id = a.artist_id)
GROUP BY
 a.artist_id,
 a.name
ORDER BY
 COUNT(sp.songplay_id) desc
```
**Getting total song plays grouped by user subscription level**
```
SELECT
 a.name AS artist_name,
 COUNT(sp.songplay_id) AS songplay_count
FROM songplays sp
JOIN artists a
 ON (sp.artist_id = a.artist_id)
GROUP BY
 a.artist_id,
 a.name
ORDER BY
 COUNT(sp.songplay_id) desc
```

**Getting a list of total song plays on a certain day of the week**
```
SELECT
 sp.*
FROM songplays sp
JOIN time t
 ON (sp.start_time = t.start_time)
WHERE t.weekday = 6
```

## Local Deployment and Execution Steps
We will be using the Redshift AWS Service for this project. Click [here](https://docs.aws.amazon.com/redshift/latest/gsg/new-user-serverless.html) to learn more about getting started with Amazon Redshift Serverless Data Warehouses.  

We will be using Infrastructure as Code (IaC) to deploy our Redshift cluster. For manual steps to setting up IAM Roles and Redshift, please refer to the documentation in this repository under [files](https://github.com/brettambrose/udacity-nanodegree-data-engineering-aws-project-redshift-data-warehouse/tree/master/files).

All executable code is written in Python.

### Note on sql_queries.py
The sql_query.py script contains the DDL and DML statements comprising the database and ETL design for this project, which includes:

1. DDL for 2 staging tables
2. DDL for 1 fact and 4 dimension tables (the Star Schema design)
3. COPY commands to bring S3 bucket data into the 2 staging tables
4. INSERT statements for populating the fact and demonsion tables
5. Basic data quality check queries
5. DROP statements to delete all tables

create_tables.py, etl.py, and dq_checks.py will use this file during their respective execution steps

### Step 1: Deploy infrastructure using IaC and finalize dwh.cfg
We are using Infrastructure as Code (IaC) to configure and deploy our Redshift cluster. 

This is accomplished by executing [cluster_deploy.py](/cluster_deploy.py), which will...
1. Create an IAM Role that has permission to use the Redshift service on AWS
2. Attach the AmazonS3ReadOnlyAccess policy to the IAM Role to allow restricted access to the udacity-dend S3 bucket
3. Populate local ~/.aws/config with IAM Role ARN
4. Create the Redshift cluster
5. Configure ingress rules on default AWS Security Group with Redshift DB port
6. Populate [dwh.cfg](/dwh.cfg) **DB_HOST** variable with the Redshift cluster endpoint
7. Validate cluster access

### Step 2: Create the staging and Star Schema tables
Execute [create_tables.py](/create_tables.py), which will import DDL and DML SQL Statements from [sql_queries.py](/sql_queries.py) to execute the following steps:
1. DROP ALL tables from the dwh database
2. CREATE 1 fact table and 4 dimension tables

### Step 3: Run the ETL to stage the S3 datasets and populate the Star Schema tables
Execute [etl.py](/etl.py). wich will import COPY and INSERT statements from [sql_queries.py](/sql_queries.py) to execute the following steps:
1. COPY from s3://udacity-dend/log-data into staging_events using the log_json_path.json metadata file
2. COPY from s3://udacity-dend/song_data into staging_songs
3. Perform transformations staging table data to INSERT into 1 fact table and 4 dimension tables

### Step 4: Perform data quality checks on the Star Schema tables
Execute [dq_checks.py](/dq_check.py), which will import SELECT statements from [sql_queries.py](/sql_queries.py) to gather and return row counts for each Data Warehouse target table.

### Step 6: Delete the IAM Role and shut down the Redshift cluster
We want to be efficient with costs associated with provisioned Redshift clusters. Given this is a self-contained project to learning purposes, we will want to delete the IAM role and Redshift cluster we spun up [Step 1](#step-1-local-setup-using-iac) after we are finished.

To do this, execute [cluster_shutdown.py](/cluster_shutdown.py)


















































Populate [AWS] KEY and SECRET with the appropriate credentials
```
[AWS]
KEY=#yourAWSKey
SECRET=#yourAWSSecret
```


Populate dwh.cfg with [AWS] KEY and SECRET values
Assuming that we have an existing **IAM User**, we will want to input

## Step 1: Local Setup using IaC
We will be using an the Redshift AWS Service for this project.  Click [here](https://docs.aws.amazon.com/redshift/latest/gsg/new-user-serverless.html) to learn more about getting started with Amazon Redshift Serverless Data Warehouses.

Assumption here is that you already have an existing **IAM User**

We will use the boto3 SDK for Python to deploy our infrastructure locally for a dc2.large, 4-node Redshift cluster configured with the 'dwh' database.

**Step 1a: Declare iam and redshift variables using your IAM User key and secret**
```
import boto3
KEY = '<your KEY here>'
SECRET = '<you SECRET here>'
IAM_ROLE_NAME = 'myRedshiftRole'
CLUSTER_IDENTIFIER = 'redshift-cluster-1'
CLUSTER_TYPE = 'multi-node'
NODE_TYPE = 'dc2.large'
NUM_NODES = 4
DB_NAME = 'dwh'
DB_USER = 'dwhuser'
DB_PASSWORD = '<your password here>'

iam = boto3.client('iam',aws_access_key_id=KEY,
                     aws_secret_access_key=SECRET,
                     region_name='us-east-1'
                  )

redshift = boto3.client('redshift',
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET,
                       region_name="us-east-1"
                       )
```

**Step 1b: Create the IAM Role that has permission to use the Redshift Service on AWS**
```
from botocore.exceptions import ClientError

try:
    dwhRole = iam.create_role(
        Path='/',
        RoleName=IAM_ROLE_NAME,
        Description = "Allows Redshift clusters to call AWS services on your behalf.",
        AssumeRolePolicyDocument=json.dumps(
            {'Statement': [{'Action': 'sts:AssumeRole',
               'Effect': 'Allow',
               'Principal': {'Service': 'redshift.amazonaws.com'}}],
             'Version': '2012-10-17'})
    )    
except Exception as e:
    print(e)
    
```

**Step 1c: Attach the AmazonS3ReadOnlyAccess policy to the IAM Role**
```
iam.attach_role_policy(RoleName=IAM_ROLE_NAME,
                       PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
                      )['ResponseMetadata']['HTTPStatusCode']
```

**Step 1d: Retrieve the IAM Role ARN (this will be the ARN value in dwh.cfg)**
```
roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']

print(roleArn)
```

**Step 1e: Create the Redshift Cluster**
This redshift cluster will have 4 nodes
```
try:
    response = redshift.create_cluster(        
        #HW
        ClusterType=CLUSTER_TYPE,
        NodeType=NODE_TYPE,
        NumberOfNodes=NUM_NODES,

        #Identifiers & Credentials
        DBName=DB_NAME,
        ClusterIdentifier=CLUSTER_IDENTIFIER,
        MasterUsername=DB_USER,
        MasterUserPassword=DB_PASSWORD,
        PubliclyAccessible=True,
        
        #Roles (for s3 access)
        IamRoles=[roleArn]
    )
except Exception as e:
    print(e)
```

**Step 1f: Retrieve the Cluster Endpoint (this will be the HOST value in dwh.cfg)**
```
myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_IDENTIFIER')['Clusters'][0]
dbEndpoint = myClusterProps['Endpoint']['Address']

print(dbEndpoint)
```

**!!!ONLY DO THE NEXT STEP WHEN THE PROJECT IS COMPLETE!!!**
**Step 1g: Delete the cluster, detach the IAM Policies, and delete the IAM Role**
```
redshift.delete_cluster( ClusterIdentifier=CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)
iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess")
iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)
```


## Step 2: Populating dwh.cfg
After completing [Local setup using IAC](#local-setup-using-iac), we will then use the information to populate the HOST and ARN values in dwh.cfg.

HOST = dbEndpoint retrieved in **Step 1d** in [Local setup using IAC](#local-setup-using-iac)
ARN = roleArn retrieved in **Step 1f** in in [Local setup using IAC](#local-setup-using-iac)

For this project, the dwh.cfg will look like this:
```
[CLUSTER]
HOST=redshift-cluster-1.cs56yxbmjkjg.us-east-1.redshift.amazonaws.com #see 
DB_NAME=dwh
DB_USER=dwhuser
DB_PASSWORD=Passw0rd
DB_PORT=5439

[IAM_ROLE]
ARN='arn:aws:iam::565095388474:role/myRedshiftRole'

[S3]
LOG_DATA='s3://udacity-dend/log-data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song_data'
```

## Step 3: Verifying data model and ETL design (sql_query.py)
The sql_query.py script contains the DDL and DML statements comprising the database and ETL design for this project, which includes:

1. DDL for 2 staging tables
2. DDL for 1 fact and 4 dimension tables (the Star Schema design)
3. COPY commands to bring S3 bucket data into the 2 staging tables
4. INSERT statements for populating the fact and demonsion tables
5. Basic data quality check queries
5. DROP statements to delete all tables

create_tables.py, etl.py, and dq_checks.py will use this file during their respective execution steps

## Step 4: Execute create_tables.py
This script will CREATE all staging, fact, and dimension tables for the Data Warehouse.  If these tables already exist, it will DROP them first before recreating them.

## Step 5: Execute etl.py
This script will load all staging, fact, and dimnesion tables for the Data Warehouse.  This script will also execute ETL success validation queries.

## Step 6: Execute dq_check.py
The script will execute basic data quality checks to ensure the ETL was successful.
